# Crawl4AI Package Documentation (Condensed Summary)

*(This file should contain a condensed summary of the key features, classes, methods, and options of the `crawl4ai` Python package, generated by `context-condenser` from the official documentation or source code.)*

## Key Components (Placeholder)

*   **`AsyncWebCrawler`:** Main class for asynchronous crawling.
    *   `__init__(urls, crawler_options)`
    *   `run()`: Starts the crawl, returns `List[CrawledData]`.
*   **`CrawlerOptions`:** Configuration object.
    *   `strategy`: 'bfs' or 'dfs'.
    *   `depth`: Max crawl depth.
    *   `limit`: Max pages to crawl.
    *   `concurrency`: Number of parallel workers.
    *   `delay`: Delay between requests (seconds).
    *   `user_agent`: Custom user agent string.
    *   `proxy`: Proxy server URL.
    *   `verify_ssl`: Boolean for SSL verification.
*   **`Filters`:** Filtering configuration.
    *   `url_filters`: List of regex patterns for allowed URLs.
    *   `content_filters`: List of functions or patterns for content filtering.
    *   `url_boundary`: Strategy for staying within domain/subdomain.
*   **`BrowserOptions`:** Browser automation settings.
    *   `headless`: Boolean (True/False).
    *   `browser_type`: 'chromium', 'firefox', 'webkit'.
    *   `page_options`: Dictionary for viewport, headers, etc.
*   **`CrawledData`:** Object returned by `run()`.
    *   `url`: The crawled URL.
    *   `content`: Extracted page content (HTML or text).
    *   `metadata`: Dictionary of page metadata.

*(Consult the full `crawl4ai` documentation for details.)*